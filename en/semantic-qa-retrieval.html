---
# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Semantic Retrieval for Question Answering Applications"
redirect_from:
- /documentation/semantic-qa-retrieval.html
---

<p>
  This document describes how to represent <a href="https://en.wikipedia.org/wiki/Word_embedding">text embedding</a>
  tensors in Vespa and how to build a scalable real time semantic search engine using Vespa's
  <a href="approximate-nn-hnsw.html">Approximate Nearest Neighbor Search Operator</a>
  to search in embedding space generated by Google's
  <a href="https://arxiv.org/abs/1907.04307">Multilingual Universal Sentence Encoder</a>.</p>
<p>
  The introduction of Transformer NLP models like <a href="https://arxiv.org/abs/1810.04805">BERT</a>
  have led to significant advancement in the state of the art for multiple tasks.
  Examples include question answering, classification and ad-hoc document ranking.
  Transformer models give the best accuracy for ranking and question answering tasks
  when used as an interaction model with cross-attention between the question and document.
  However, running online inference with query and document cross-attention models over large document collections
  is computationally prohibitively expensive.
  The problematic computational complexity of inference using Transformer models over large collections
  has led to increased interest in multi-stage retrieval and ranking architectures.
  The first stage retrieves candidate documents using a more cost-efficient scoring function
  and the advanced cross-attention model inference is limited to the top ranking documents from the first stage.
</p>
<p>
  In <a href="https://arxiv.org/abs/1907.04780">ReQA: An Evaluation for End-to-End Answer Retrieval Models</a>,
  Ahmad et al. introduce <em>Retrieval Question Answering (ReQA)</em>,
  a benchmark for evaluating large-scale sentence level answer retrieval models.
  There, they establish a baseline for both traditional information retrieval (sparse term based)
  and neural (dense) encoding models on the <em>Stanford Question Answering Dataset (SQuAD)</em> v1.1 dataset.
</p>
<p>
  In this document, we reproduce the work done by Ahmad et al. on the
  <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD 1.1</a> retrieval task using Vespa serving engine.
  We replicate the results from the mentioned paper,
  which enables organizations to deploy state-of-the-art question answering retrieval systems
  with low effort using the scalable Vespa engine.
</p>
<p>
  Vespa has support for storing and indexing dense tensors field types along with traditional string fields
  with support for traditional sparse term based text ranking features
  like <a href="reference/bm25.html">bm25</a> or Vespa's <a href="reference/nativerank.html">nativeRank</a>.
  Having both traditional text ranking features and semantic similarity features expressed in the same engine
  is a powerful feature of Vespa, which enables hybrid retrieval using both sparse and dense representation.
</p>
<p>
  The work described in this document can be reproduced using the
  <a href="https://github.com/vespa-engine/sample-apps/tree/master/semantic-qa-retrieval">
    semantic-qa-retrieval sample application</a>.
</p>



<h2 id="universal-sentence-encoder">About Google's Universal Sentence Encoder</h2>
<p>
  The <em>Universal Sentence Encoder</em> encodes text into fixed length dense embedding space
  that can be used for broad range of tasks such as semantic similarity,
  semantic retrieval and other natural language processing (NLP) tasks.
  Google has released more sentence encoder models with different goals,
  and following the work of Ahmad et al. we use the
  <a href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html">
    Multilingual Universal Sentence Encoder for Question-Answer Retrieval</a>.
</p>
<p>
  The Universal Sentence Encoder for Question-Answer Retrieval enables us
  to process questions and candidate answer sentences independently
  and map the high dimensional sparse text representation
  to a relatively low dimensional dense tensor representation,
  where we can use Vespa's approximate nearest neighbor search operator to retrieve documents efficiently.
</p>
<ul>
  <li>Question text is encoded using the question encoder,
    which takes the question text as input and outputs a 512 dimensional dense tensor.</li>
  <li>Each sentence of text is encoded using the response encoder,
    which takes the sentence and the surrounding context (e.g. paragraph level) as input
    and outputs a 512 dimension dense tensor.</li>
</ul>
<p>
  We can store and index the dense tensor embedding in Vespa using <a href="tensor-user-guide.html">tensor fields</a>
  and use Vespa's approximate nearest neighbor search operator to retrieve documents.
</p>
<figure>
<img src="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png"
     alt="Illustration courtesy of https://tfhub.dev/google/universal-sentence-encoder/2" width="1272" height="280"/>
  <figcaption>
    Illustration courtesy of <a href="https://tfhub.dev/google/universal-sentence-encoder/2">
    https://tfhub.dev/google/universal-sentence-encoder/2</a>
  </figcaption>
</figure>
<p>Papers and resources on Google's Universal Sentence Encoder:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1907.04307">
    Multilingual Universal Sentence Encoder for Semantic Retrieval Paper</a> </li>
  <li><a href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html">
    Google AI Blog: Multilingual Universal Sentence Encoder for Semantic Retrieval</a> </li>
  <li><a href="https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1">
    Tensorflow Hub: USE-QA </a> </li>
</ul>
<p>
  A similar dual encoder architecture (question, document) is described in the
  <a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a>
  by Facebook Research,
  where they demonstrate how a trained dense representation using a dual question encoder
  based on BERT outperforms traditional IR retrieval - quote:
  <em>When evaluated on a wide range of open-domain QA datasets,
    our dense retriever outperforms a strong Lucene-BM25 system
    largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy,
    and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.</em>
</p>



<h2 id="dataset">About the SQuAD dataset</h2>
<p>
  The <a href="https://arxiv.org/abs/1606.05250">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>
  paper introduced the SQuAD dataset which is available for download at
  <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD-explorer</a>.
</p>
<p>
  The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset,
  consisting of questions posed by crowdworkers on a set of Wikipedia articles,
  where the answer to every question is a segment of text, or span, from the corresponding reading passage.
  In our experiments we use the train v1.1 dataset.
</p>
<p>
  Sample questions and answers for a given paragraph context taken from a snapshot of the
  <a href="https://en.wikipedia.org/wiki/University_of_Notre_Dame">University_of_Notre_Dame Wikipedia page</a>:
</p>
<pre>{% highlight json %}
{
    "data": [
        {
            "title": "University_of_Notre_Dame",
            "paragraphs": [
                {
                    "context": "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
                    "qas": [
                        {
                            "question": "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?",
                            "answers": [
                                {
                                    "answer_start": 515,
                                    "text": "Saint Bernadette Soubirous"
                                }
                            ],
                            "id": "5733be284776f41900661182"
                        },
                        {
                            "question": "What is in front of the Notre Dame Main Building?",
                            "answers": [
                                {
                                    "answer_start": 188,
                                    "text": "a copper statue of Christ"
                                }
                            ],
                            "id": "5733be284776f4190066117f"
                        }
                    ]
                }
            ]
        }
    ]
}
{% endhighlight %}</pre>
<p>
  The <em>answer_</em> start represents the offset where the answer for the question can be found.
  The SQuAD v1.1 train dataset consists of 87,599 questions and 18,896 paragraphs.
  The paragraphs can further be segmented into 91,729 sentences using a sentence tokenizer.
</p>



<h2 id="squad-data-modelling-with-vespa">SQuAD Data modelling with Vespa</h2>
<p>
  We model the SQuaD data set in Vespa in two different document schema types;
  a <em>context</em> document type and a <em>sentence</em> document type:
</p>
<p>Context document type:</p>
<pre>
schema context {
    document context {
        field context_id type int {
            indexing: summary | attribute
        }
        field text type string {
            indexing: summary | index
            index: enable-bm25
        }
    }
}
</pre>
<p>Sentence document type:</p>
<pre>
schema sentence {
    document sentence inherits context {
        field sentence_embedding type tensor&lt;float&gt;(x[512]) {
            indexing: attribute
            attribute {
                distance-metric: euclidean
            }
            index {
                hnsw {
                    max-links-per-node: 16
                    neighbors-to-explore-at-insert: 500
                }
            }
        }
    }
}
</pre>
<p>
  See <a href="approximate-nn-hnsw.html">Approximate Nearest Neighbor Search using HNSW Index</a>
  for details on the <em>HNSW</em> index settings and <em>distance-metric</em>.
  In this case, we use the <em>euclidean</em> distance metric.
</p>



<h2 id="converting-the-squad-json-to-vespa-json-feed-format">Converting the SQuAD json to Vespa json feed format</h2>
<p>
  In order to feed the SQuAD data, we need to convert it into our Vespa document schema
  and feed documents using the <a href="reference/document-json-format.html">Vespa json format</a>.
</p>
<p>
  For each paragraph context we run a simple
  <a href="https://github.com/google/retrieval-qa-eval/blob/master/sb_sed.py">sentence tokenizer</a>
  published by Ahmed et al. to extract sentences from the paragraph context.
  We simply assign a unique sentence id sentences and likewise for context.
  Sentence sample extracted from the above example paragraph:
</p>
<pre>{% highlight json %}
{
    "put": "id:squad:sentence::5",
    "fields": {
        "context_id": 0,
        "text": "Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".",
        "sentence_embedding": {
            "values": [
                -0.0528511106967926,
                0.00927420798689127,
                .... ,
                0.011870068497955799,
                -0.06848619878292084
            ]
        }
    }
}
{% endhighlight %}</pre>
<p>
  We can feed the generated document set to our Vespa instance using any of the feed APIs,
  here using <a href="vespa-cli.html">vespa feed</a>.
  After this step, we have one content db with 18,896 context documents
  and 91,729 sentences in another in the same Vespa content cluster.
</p>



<h2 id="sentence-and-paragraph-retrieval">Sentence and Paragraph Retrieval</h2>
<p>
  The goal of the ReQA task is to retrieve sentences which have the answer for any given question.
  We can also compute context or paragraph level retrieval using the sentence level semantic similarity
  by aggregating over the sentence level scores.
  We can do this efficiently using the Vespa <a href="grouping.html">grouping</a> API
  if we want to retrieve paragraphs instead of sentences.
</p>
<p>
  We use the Vespa <a href="query-api.html">Query API</a> to express our query request logic.
  We use the <a href="query-language.html">YQL</a> query language to express our query retrieval logic.
</p>
<p>
  Using the sample question from the example, the POST HTTP query request is:
</p>
<pre>{% highlight json %}
{
    "yql": "select * from sources sentence where ({targetHits:100}nearestNeighbor(sentence_embedding,query_embedding))",
    "hits": 100,
    "input.query(query_embedding)": [-0.0466, ...,0.064],
    "ranking.profile": "sentence-semantic-similarity"
}
{% endhighlight %}</pre>
<ul>
  <li>We use the nearestNeighbor search operator to retrieve the closest 100 sentences in embedding space
    using euclidean distance as configured with the tensor HNSW settings.</li>
  <li>The top 100 top nearest sentences are ranked by the rank profile passed in the <em>ranking.profile</em>.</li>
  <li>The dense tensor representation encoded by the sentence encoder is passed
    by the <em>input.query(query_embedding)</em> parameter.</li>
</ul>
<p>
  The rank profile is defined in the <em>sentence</em>
  <a href="https://github.com/vespa-engine/sample-apps/blob/master/semantic-qa-retrieval/src/main/application/schemas/sentence.sd#L26">
    document schema</a>:
</p>
<pre>
rank-profile sentence-semantic-similarity inherits default {
    first-phase {
        expression: closeness(sentence_embedding)
    }
}
</pre>
<p>
  Where <em>closeness</em> is a Vespa <a href="reference/rank-features.html">ranking feature</a>
  which is defined as <em>1/(1 + distance)</em>.
</p>
<p>
  For paragraph level retrieval we use Vespa's <a href="grouping.html">grouping</a> feature
  to retrieve paragraphs instead of sentences.
  As in the paper, we use the max sentence score in the paragraph to represent the paragraph level score.
  The query above is changed to add a grouping specification:
</p>
<pre>{% highlight json %}
{
    "yql": "select * from sources sentence where ({targetHits:100}nearestNeighbor(sentence_embedding,query_embedding)) | all(group(context_id) max(100) order(-max(relevance())) each( max(2) each(output(summary())) as(sentences)) as(paragraphs))",
    "hits": 0,
    "input.query(query_embedding)": [-0.0466, ...,0.064],
    "ranking.profile": "sentence-semantic-similarity"
}
{% endhighlight %}</pre>
<p>
  The grouping expression groups sentences by the context id,
  and order the groups (paragraphs) by the maximum rank score.
  For each unique context id, we get the top ranking sentences ordered by their rank score,
  assigned by the chosen rank profile.
</p>



<h2 id="hybrid-retrieval-using-both-dense-encoding-and-sparse-term-representation">
  Hybrid retrieval using both dense (encoding) and sparse (term) representation</h2>
<p>
  We can also retrieve using a hybrid combination consisting of dense retrieval and regular query term matching:
</p>
<pre>{% highlight json %}
{
    "yql": "select * from sources sentence  where ({targetHits:100}nearestNeighbor(sentence_embedding,query_embedding)) or userQuery()",
    "query": "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?",
    "type": "any",
    "hits": 100,
    "input.query(query_embedding)": [-0.0466, ...,0.064],
    "ranking.profile": "bm25-sentence-semantic-similarity"
}
{% endhighlight %}</pre>
<p>
  We use logical disjunction to combine the nearest neighbor query operator retrieving in dense embedding space
  with the regular term based (sparse) retrieval.
  We use a simple linear combination of the bm25 score on text and the previously described closeness ranking feature:
</p>
<pre>
rank-profile bm25-sentence-semantic-similarity inherits default {
    first-phase {
        expression: bm25(text) + closeness(sentence_embedding)
    }
}
</pre>
